{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential, Model\n",
    "from tensorflow.keras.layers import Conv2D, MaxPool2D, GlobalAveragePooling2D\n",
    "from tensorflow.keras.layers import Dense, Flatten, BatchNormalization, Activation, Dropout\n",
    "from tensorflow.keras.layers import Conv1D, Embedding, GlobalAveragePooling1D \n",
    "from tensorflow.keras.optimizers import Adam, RMSprop\n",
    "from tensorflow.keras.preprocessing import image\n",
    "\n",
    "import string\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./datasets/memotion_dataset_7k/labels_img.csv')\n",
    "df.drop(df.columns[df.columns.str.contains('unnamed', case=False)], axis=1, inplace=True)\n",
    "df = df.drop(columns = ['text_ocr', 'humour', 'sarcasm', 'offensive', 'motivational'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(inplace=True)\n",
    "df.isnull().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "width = 100\n",
    "height = 100\n",
    "img_vet = []\n",
    "for i in tqdm(range(df.shape[0])):\n",
    "    path = './datasets/memotion_dataset_7k/images/' + df['image_name'][i]\n",
    "    img = image.load_img(path, target_size=(width, height, 3))\n",
    "    img = image.img_to_array(img)\n",
    "    img = img/255.0\n",
    "    img_vet.append(img)\n",
    "        \n",
    "img_vet = np.array(img_vet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_vet.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "plt.axis(\"off\")\n",
    "for i in range(9):\n",
    "    ax = plt.subplot(3, 3, i + 1)\n",
    "    plt.imshow(img_vet[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sentiment = df['overall_sentiment']\n",
    "df_sentiment = pd.get_dummies(df_sentiment)\n",
    "df_sentiment.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(img_vet, df_sentiment, test_size = 0.2, stratify=df_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_augmentation = tf.keras.Sequential([\n",
    "  tf.keras.layers.experimental.preprocessing.RandomFlip('horizontal'),\n",
    "  tf.keras.layers.experimental.preprocessing.RandomContrast([.5,2]),\n",
    "  tf.keras.layers.experimental.preprocessing.RandomRotation(0.2),\n",
    "  tf.keras.layers.experimental.preprocessing.RandomZoom(0.1)\n",
    "])\n",
    "\n",
    "preprocess_input = tf.keras.applications.resnet_v2.preprocess_input\n",
    "rescale = tf.keras.layers.experimental.preprocessing.Rescaling(1./127.5, offset= -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "for i in range(9):\n",
    "  augmented_image = data_augmentation(img_vet)\n",
    "  ax = plt.subplot(3, 3, i + 1)\n",
    "  plt.imshow(augmented_image[0])\n",
    "  plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model_1 = tf.keras.applications.ResNet50(input_shape=img_vet[0].shape,\n",
    "                                               include_top=False,\n",
    "                                               weights='imagenet')\n",
    "base_model_2 = tf.keras.applications.VGG16(input_shape=img_vet[0].shape,\n",
    "                                               include_top=False,\n",
    "                                               weights='imagenet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model_1.trainable = False\n",
    "base_model_2.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_model():\n",
    "    image_input = tf.keras.Input(shape=(100, 100, 3), name = 'image_input')\n",
    "    image_layers = data_augmentation(image_input)\n",
    "    image_layers = preprocess_input(image_layers)\n",
    "    layer_bm_1 = base_model_1(image_input, training=False)\n",
    "    layer_bm_1 = Conv2D(2048, kernel_size=2,padding='valid')(layer_bm_1)\n",
    "    layer_bm_1 = Dense(512)(layer_bm_1)\n",
    "    layer_bm_2 = base_model_2(image_input, training=False)\n",
    "    layer_bm_2 = Dense(512)(layer_bm_2)\n",
    "    layers = tf.keras.layers.concatenate([layer_bm_1, layer_bm_2])\n",
    "    image_layers = GlobalAveragePooling2D()(layers)\n",
    "    image_layers = Dropout(0.2, name = 'dropout_layer')(image_layers)\n",
    "    return image_input, image_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_input, image_layers = image_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardization(data):\n",
    "    data = data.apply(lambda x: x.lower())\n",
    "    data = data.apply(lambda x: re.sub(r'\\d+', '', x))\n",
    "    data = data.apply(lambda x: re.sub(r'.com', '', x, flags=re.MULTILINE))\n",
    "    data = data.apply(lambda x: x.translate(str.maketrans('', '', string.punctuation)))\n",
    "    return data\n",
    "\n",
    "df['text_corrected'] = standardization(df.text_corrected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
    "vocab_size = 10000\n",
    "sequence_length = 50\n",
    "\n",
    "vectorize_layer = TextVectorization(\n",
    "    max_tokens=vocab_size,\n",
    "    output_mode='int',\n",
    "    output_sequence_length=sequence_length)\n",
    "\n",
    "text_ds = np.asarray(df['text_corrected'])\n",
    "vectorize_layer.adapt(tf.convert_to_tensor(text_ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_text_train, X_text_test, y_text_train, y_text_test = train_test_split(df.text_corrected, df_sentiment, \\\n",
    "                                                                        test_size = 0.2, stratify=df_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim=256\n",
    "\n",
    "def text_model():\n",
    "    text_input = tf.keras.Input(shape=(None,), dtype=tf.string, name='text_input')\n",
    "    text_layers = vectorize_layer(text_input)\n",
    "    text_layers = tf.keras.layers.Embedding(vocab_size, embedding_dim, name=\"embedding\")(text_layers)\n",
    "\n",
    "    text_layers = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(512, activation='relu', return_sequences=True))(text_layers)\n",
    "    text_layers = tf.keras.layers.BatchNormalization()(text_layers)\n",
    "    text_layers = tf.keras.layers.Bidirectional(tf.keras.layers.GRU(512, activation='relu', return_sequences=True))(text_layers)\n",
    "    text_layers = tf.keras.layers.BatchNormalization()(text_layers)\n",
    "\n",
    "    text_layers = tf.keras.layers.Conv1D(128, 7, padding=\"valid\", activation=\"relu\", strides=3)(text_layers)\n",
    "    text_layers = tf.keras.layers.Conv1D(128, 7, padding=\"valid\", activation=\"relu\", strides=3)(text_layers)\n",
    "    \n",
    "    text_layers = tf.keras.layers.GlobalMaxPooling1D()(text_layers)\n",
    "\n",
    "    text_layers = tf.keras.layers.Dense(2048, activation=\"relu\")(text_layers)\n",
    "    text_layers = tf.keras.layers.Dropout(0.5)(text_layers)\n",
    "    return text_input, text_layers\n",
    "\n",
    "text_input, text_layers = text_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(layer_1, layer_2, image_input, text_input):\n",
    "    concatenate = tf.keras.layers.concatenate([layer_1, layer_2], axis=1)\n",
    "    semi_final_layer = tf.keras.layers.Dense(2048, activation='softmax')(concatenate)\n",
    "\n",
    "    prediction_layer = tf.keras.layers.Dense(5, activation='sigmoid', name = 'classify')\n",
    "\n",
    "    output = prediction_layer(semi_final_layer)\n",
    "\n",
    "    model = tf.keras.Model(inputs = [image_input, text_input] , \n",
    "                           outputs = output)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model(image_layers, text_layers, image_input, text_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Define the checkpoint directory to store the checkpoints\n",
    "checkpoint_dir = './datasets/memotion_dataset_7k/training_checkpoints'\n",
    "\n",
    "# Name of the checkpoint files\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for decaying the learning rate.\n",
    "# You can define any decay function you need.\n",
    "def decay(epoch):\n",
    "  if epoch < 3:\n",
    "    return 1e-1\n",
    "  elif epoch >= 3 and epoch < 5:\n",
    "    return 1e-2\n",
    "  else:\n",
    "    return 1e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callback for printing the LR at the end of each epoch.\n",
    "class PrintLR(tf.keras.callbacks.Callback):\n",
    "  def on_epoch_end(self, epoch, logs=None):\n",
    "    print('\\nLearning rate for epoch {} is {}'.format(epoch + 1,\n",
    "                                                      model.optimizer.lr.numpy()))\n",
    "\n",
    "callbacks = [\n",
    "    tf.keras.callbacks.TensorBoard(log_dir='./logs'),\n",
    "    tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_prefix,\n",
    "                                       save_weights_only=True),\n",
    "    tf.keras.callbacks.LearningRateScheduler(decay),\n",
    "    PrintLR()\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=tf.keras.optimizers.RMSprop(),\n",
    "              loss = tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['binary_accuracy', 'accuracy'])\n",
    "\n",
    "history = model.fit(x = {\"image_input\": X_train, \"text_input\": X_text_train},\n",
    "                    y = y_train,\n",
    "                    batch_size=256,\n",
    "                    epochs=25,\n",
    "                    callbacks=callbacks\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_history = pd.DataFrame(history.history)\n",
    "df_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1,2, figsize=(15, 5))\n",
    "fig.tight_layout(pad=5.0)\n",
    "\n",
    "axes[0].plot(df_history.loss)\n",
    "axes[0].set_xlabel('No of Epochs')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].set_title('Loss vs Epochs')\n",
    "\n",
    "axes[1].plot(df_history.binary_accuracy)\n",
    "axes[1].plot(df_history.accuracy)\n",
    "axes[1].set_xlabel('No of Epochs')\n",
    "axes[1].set_ylabel('Loss')\n",
    "axes[1].set_title('Accuracy vs Epochs')\n",
    "axes[1].legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_ = model.evaluate(x = {\"image_input\": X_test, \"text\": X_text_test},\n",
    "                    y = y_test,\n",
    "                    batch_size=32,\n",
    "                    verbose=1\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = model.predict(x = {\"image_input\": X_test, \"text\": X_text_test})\n",
    "prediction = np.array(prediction)\n",
    "prediction = np.squeeze(prediction)\n",
    "prediction = 1/(1+np.exp(-np.array(prediction)))\n",
    "prediction = np.where(prediction > 0.5, 1, 0)\n",
    "y_true = y_test.values\n",
    "\n",
    "micro_f1_score = f1_score(y_true[:,1], prediction[:,1], average='micro')\n",
    "macro_f1_score = f1_score(y_true[:,1], prediction[:,1], average='macro')\n",
    "\n",
    "print(\"Micro F1 score for Task A is \", micro_f1_score)\n",
    "print(\"Macro F1 score for Task A is \", macro_f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "fig, axes = plt.subplots(1,2, figsize=(20, 4))\n",
    "fig.tight_layout(pad=5.0)\n",
    "\n",
    "\n",
    "x = list(y_test.columns)\n",
    "\n",
    "axes[0].imshow(X[random.randint(0,X_test.shape[0]),:,:,:])\n",
    "\n",
    "axes[1].bar(x, model.predict(x = {\"image_input\": X_test, \"text\": X_text_test})[random.randint(0,X_test.shape[0]),:])\n",
    "axes[1].set_xlabel('Labels')\n",
    "axes[1].set_ylabel('Probanility')\n",
    "axes[1].set_title('Humuor Prob.')\n",
    "axes[1].set_xticks(x)\n",
    "axes[1].set_ylim(0,1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
